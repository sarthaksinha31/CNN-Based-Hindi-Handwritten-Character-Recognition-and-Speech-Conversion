{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np                                \n",
    "import matplotlib.pyplot as plt\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten \n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution2D,Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "from keras.utils import multi_gpu_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1048640   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 1,441,646\n",
      "Trainable params: 1,440,558\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inp = (32,32,1)\n",
    "X_input = Input(inp)\n",
    "X = Conv2D(32, (3, 3), activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.001))(X_input)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Conv2D(64, (3, 3), activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.001))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = MaxPooling2D(pool_size=(2, 2))(X)   # reduces to 14x14x32\n",
    "\n",
    "X = Conv2D(128, (3, 3), activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.001))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Conv2D(256, (3, 3), activation='relu',padding='same',kernel_regularizer=regularizers.l2(0.001))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = MaxPooling2D(pool_size=(2, 2))(X)   # reduces to 7x7x64 = 3136 neurons\n",
    "\n",
    "X = Flatten()(X)                        \n",
    "X = Dense(64, activation='relu')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Dense(46, activation='softmax')(X)\n",
    "model = Model(inputs=X_input, outputs= X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "inputs = Input(shape = (32,32,1))\n",
    "conv1 = Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
    "conv2 = Conv2D(128, 3, padding='same', activation='relu')(conv1)\n",
    "pool2 = MaxPooling2D((2,2))(conv2)\n",
    "conv3 = Conv2D(128, 3, padding='same', activation='relu')(pool2)\n",
    "conv4 = Conv2D(256, 5, padding='same', activation='relu')(conv3)\n",
    "pool4 = MaxPooling2D((2,2))(conv4)\n",
    "conv5 = Conv2D(256, 5, padding='same', activation='relu')(pool4)\n",
    "flat = Flatten()(conv5)\n",
    "dense0 = Dense(512, activation='relu')(flat)\n",
    "dense1 = Dense(128, activation='relu')(dense0)\n",
    "dense2 = Dense(46, activation='softmax')(dense1)\n",
    "\n",
    "model = Model(input = inputs, output = dense2)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78200 images belonging to 46 classes.\n",
      "Found 13800 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "transform = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = transform.flow_from_directory('F:/DevanagariHandwrittenCharacterDataset/Train/',target_size=(32,32),batch_size=4096,color_mode='grayscale')\n",
    "test_generator = transform.flow_from_directory('F:/DevanagariHandwrittenCharacterDataset/Test/',target_size=(32,32),batch_size=256,color_mode='grayscale',shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath=\"model.h5\"\n",
    "model = load_model(filepath)\n",
    "adm=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = [callbacks.ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 476s 2s/step - loss: 0.2130 - val_loss: 0.4012\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acharyaece/anaconda3/envs/tensorflowenv/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 243s 1s/step - loss: 0.0460 - val_loss: 1.0638\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 241s 1s/step - loss: 0.0349 - val_loss: 0.0782\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 240s 1s/step - loss: 0.0263 - val_loss: 0.0946\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 238s 1s/step - loss: 0.0247 - val_loss: 0.0576\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 241s 1s/step - loss: 0.0222 - val_loss: 1.7910\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 241s 1s/step - loss: 0.0207 - val_loss: 0.0887\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 242s 1s/step - loss: 0.0177 - val_loss: 0.0553\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 240s 1s/step - loss: 0.0162 - val_loss: 9.0650\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 240s 1s/step - loss: 0.0195 - val_loss: 0.0698\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 238s 1s/step - loss: 0.0140 - val_loss: 0.2457\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 239s 1s/step - loss: 0.0160 - val_loss: 0.0591\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 243s 1s/step - loss: 0.0129 - val_loss: 0.0907\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 240s 1s/step - loss: 0.0096 - val_loss: 0.0556\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 241s 1s/step - loss: 0.0102 - val_loss: 0.0594\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 249s 1s/step - loss: 0.0113 - val_loss: 0.1525\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 242s 1s/step - loss: 0.0080 - val_loss: 0.0651\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 239s 1s/step - loss: 0.0090 - val_loss: 0.0610\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 244s 1s/step - loss: 0.0077 - val_loss: 0.2641\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 241s 1s/step - loss: 0.0086 - val_loss: 0.0570\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=2)\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='rmsprop')\n",
    "history = parallel_model.fit_generator(train_generator, steps_per_epoch = 200 , epochs=20, validation_data = test_generator, \n",
    "                              validation_steps= 50,callbacks=call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 10s 96ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.059120364725104554, 0.988454915653114]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator,verbose=1,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"model.h5\"\n",
    "model =load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 12s 123ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746516963208952, 0.9911838790931989]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator,verbose=1,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"F:/DevanagariHandwrittenCharacterDataset/Test/character_13_daa/10675.png\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_1_ka\n",
      "character_2_kha\n",
      "character_3_ga\n",
      "character_4_gha\n",
      "character_5_kna\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyttsx3 as pyttsx\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "#img = cv2.imread(\"C:/Users/WINDOW 10/Desktop/threshold/faizal.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "#img = cv2.imread(\"F:/DevanagariHandwrittenCharacterDataset/Test/character_13_daa/10675.png\",cv2.IMREAD_GRAYSCALE)\n",
    "for i in range(1,6):\n",
    "    img = cv2.imread(str(i)+\".jpg\",0)\n",
    "    #img1 = cv2.imread(str(i)+\".jpg\",0)\n",
    "    ret,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)\n",
    "    img = cv2.dilate(img,kernel,iterations = 1)\n",
    "    resized = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA)\n",
    "    resized = resized.astype('float32')\n",
    "    resized=resized/255\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    resized.shape = (-1,32,32,1)\n",
    "    a=model.predict(resized)\n",
    "    m=test_generator.class_indices\n",
    "    char = list(m.keys())[list(m.values()).index(a.argmax(axis=-1))]\n",
    "    print(char)\n",
    "    resized.shape = (32,32,1)\n",
    "    cv2.imshow('image',resized)\n",
    "    engine = pyttsx.init()\n",
    "    engine.setProperty('rate', 200)\n",
    "    engine.say('The Character is %s' %(char))\n",
    "    engine.runAndWait()\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    key = cv2.waitKey(2000)#pauses for 3 seconds before fetching next image\n",
    "    #if key == 27:#if ESC is pressed, exit loop\n",
    "    cv2.destroyAllWindows()\n",
    "     #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.jpg'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1)+\".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"1.jpg\",0)\n",
    "cv2.imshow('image',img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 32, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'character_11_taamatar'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character_10_yna': 0,\n",
       " 'character_11_taamatar': 1,\n",
       " 'character_12_thaa': 2,\n",
       " 'character_13_daa': 3,\n",
       " 'character_14_dhaa': 4,\n",
       " 'character_15_adna': 5,\n",
       " 'character_16_tabala': 6,\n",
       " 'character_17_tha': 7,\n",
       " 'character_18_da': 8,\n",
       " 'character_19_dha': 9,\n",
       " 'character_1_ka': 10,\n",
       " 'character_20_na': 11,\n",
       " 'character_21_pa': 12,\n",
       " 'character_22_pha': 13,\n",
       " 'character_23_ba': 14,\n",
       " 'character_24_bha': 15,\n",
       " 'character_25_ma': 16,\n",
       " 'character_26_yaw': 17,\n",
       " 'character_27_ra': 18,\n",
       " 'character_28_la': 19,\n",
       " 'character_29_waw': 20,\n",
       " 'character_2_kha': 21,\n",
       " 'character_30_motosaw': 22,\n",
       " 'character_31_petchiryakha': 23,\n",
       " 'character_32_patalosaw': 24,\n",
       " 'character_33_ha': 25,\n",
       " 'character_34_chhya': 26,\n",
       " 'character_35_tra': 27,\n",
       " 'character_36_gya': 28,\n",
       " 'character_3_ga': 29,\n",
       " 'character_4_gha': 30,\n",
       " 'character_5_kna': 31,\n",
       " 'character_6_cha': 32,\n",
       " 'character_7_chha': 33,\n",
       " 'character_8_ja': 34,\n",
       " 'character_9_jha': 35,\n",
       " 'digit_0': 36,\n",
       " 'digit_1': 37,\n",
       " 'digit_2': 38,\n",
       " 'digit_3': 39,\n",
       " 'digit_4': 40,\n",
       " 'digit_5': 41,\n",
       " 'digit_6': 42,\n",
       " 'digit_7': 43,\n",
       " 'digit_8': 44,\n",
       " 'digit_9': 45}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=model.predict(resized)\n",
    "a.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
